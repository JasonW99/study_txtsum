## textSum using the toy data set
### 1. create a working directory, and copy the "textsum" folder to that directory

### 2. build the project

```bash
touch WORKSPACE
bazel build -c opt --config=cuda textsum/...        # if using GPU
bazel build -c opt textsum/...                      # if using CPU
```

### 3. use the toy dataset to generate data for training and testing

```bash
# copy the date to the working directory
cp -r ./textsum/data data
# download the new vocabulary for the toy data set
wget -O data/new_vocab https://github.com/tensorflow/models/files/499491/vocab 
# convert the toy data from binary format to text format
python textsum/data_convert_example.py --command binary_to_text --in_file data/data --out_file data/text_data

```
use the first data pair in the "text_data" to create file the "text_data_test".

```bash
abstract=<d> <p> <s> sri lanka closes schools as war escalates . </s> </p> </d>	article=<d> <p> <s> the sri lankan gove
... ...
... ...
she described as the biggest ever drive to take the tiger town of jaffna . . </s> </p> </d>	publisher=AFP
```
and use the rest content in the "text_data" to create the file "text_data_train".
then convert both "text_data_train" and "text_data_test" into binary format

```bash
python textsum/data_convert_example.py --command text_to_binary --in_file data/text_data_train --out_file data/bin_data_train
python textsum/data_convert_example.py --command text_to_binary --in_file data/text_data_test --out_file data/bin_data_test

```

### 4. train with the toy data

```bash
# Run the training
bazel-bin/textsum/seq2seq_attention \
     --mode=train \
     --article_key=article \
     --abstract_key=abstract \
     --data_path=data/bin_data_train\
     --vocab_path=data/new_vocab \
     --log_root=textsum/log_root \
     --train_dir=textsum/log_root/train \
     --max_run_steps=100 
```

The command line parameters are:

i.    mode –  specify model training, other options are evaluate and decode

ii.   article_key –  the tag indicating article labels in the text

iii.  abstract_key – the tag indicating summarized abstract labels in the text. In Gigaword text, it is set as ‘headline’

iv.   data_path – the file path for binary training data

v.    vocab_path – the file path for vocabulary data (text file)

vi.   log_root – the directory path to store the logs. Textsum algorithm frequently saves intermediate steps in the log_root directory.

vii.  train_dir – the directory to output model training summary

There are also model training parameters specified in python code seq2seq_attention.py:

i.    max_run_steps – the maximum number of epochs to train

ii.   max_article_sentences – Tensorflow Textsum model only uses the first 2 sentences of an article for training, under assumption that the first 2 sentences contain enough information for summarization.

iii.  max_abstract_sentences – Tensorflow Textsum model only uses the first 100 words of abstracts as target words to train the model.  Similarly, feasibility and impact of increasing this value needs further investigation.

iv.   beam_size – The beam_size limits the number of potential sequences to be searched for optimal summarization.  An analogue to this is the number of optimal & suboptimal sequence generated by Dynamic programming (Viterbi algorithm).  For example, when the beam_size is 4, the decode algorithm will generate 4 best summaries.

v.    eval_interval_secs – By default is 60 secs.  Evaluation is running simultaneously at model training time to monitor the quality of training process.

vi.   checkpoint_secs – During training, the algorithm frequently outputs intermediate results to log_root folder.  This parameter controls the outputting frequency.  It is not clear whether the algorithm can use previously saved states to continue training.

vii. num_gpus – The number of GPU.  For multi-gpu settings, this number should be larger than 1 to fully unleash the power. Normally it should be 1.

viii. minimum learning rate & learning rate – These two parameters are set in seq2seq_attention_model.HParams.  For nerual network training, these are important hyper-parameters that affect the convergence.

ix.   Encoder layers – number of layers in neural network

x.    num_hidden – number of RNN cells

xi.   emb_dim – the dimensionality of word embeddings

### 5. test with the toy data

```bash
# Run the test
bazel-bin/textsum/seq2seq_attention \
     --mode=decode \
     --article_key=article \
     --abstract_key=abstract \
     --data_path=data/bin_data_test\
     --vocab_path=data/new_vocab \
     --log_root=textsum/log_root \
     --decode_dir=textsum/log_root/decode \
     --beam_size=6
```
### 6. notes
the "textsum" directory is from the tensorflow repository "tensorflow/models/research/textsum"








